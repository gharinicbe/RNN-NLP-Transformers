{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d869eae7",
   "metadata": {},
   "source": [
    "## What is Textblob\n",
    "\n",
    "* Textblob is an open-source python library used to perform NLP activities like Lemmatization, Stemming, Tokenization, Noun Phrase Extraction, POS Tagging, N-Grams, Sentiment Analysis. \n",
    "\n",
    "* It is faster than NLTK, however it does not provide the functionalities like vectorization, dependency parsing.\n",
    "\n",
    "* Text Classification, Sentiment Analysis can be performed using Textblob. \n",
    "* Official Link to Textblob is: https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "* Installation: pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea0c543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Install Textblob\n",
    "#!pip install nltk\n",
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51aa56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5213c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a807ca3",
   "metadata": {},
   "source": [
    "### Functionalities of Textblob\n",
    "* Language Traslation\n",
    "* Word Correction\n",
    "* Word Count\n",
    "* Phrase Extraction\n",
    "* POS Tagging\n",
    "* Tokenization\n",
    "* Plularization of words using Textblob\n",
    "* Lemmatization using Textblob\n",
    "* n-gram in Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e4f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\harinilearn\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\harinilearn\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\harinilearn\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\harinilearn\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\harinilearn\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\harinilearn\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\harinilearn\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Harini\n",
      "[nltk_data]     Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Harini\n",
      "[nltk_data]     Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Harini\n",
      "[nltk_data]     Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Harini Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to C:\\Users\\Harini\n",
      "[nltk_data]     Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Harini\n",
      "[nltk_data]     Balaji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "! pip install -U textblob\n",
    "! python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62435ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "டெவலப்பர்களுக்கு TextBlob ஒரு சிறந்த கருவி\n",
      "टेक्स्टब्लॉब डेवलपर्स के लिए एक बेहतरीन टूल है\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob('TextBlob is a great tool for developers')\n",
    "print(blob.translate(from_lang='en', to='ta'))\n",
    "print(blob.translate(from_lang='en', to='hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fdb42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text in Spanish: Hola Juan, ¿cómo estás?\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Hey John, How are You\")\n",
    " \n",
    "print(\"Input text in Spanish:\",blob.translate(from_lang='en', to='es'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe5f92",
   "metadata": {},
   "source": [
    "#### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf80d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\" ABCD Corp alays values ttheir employees!!!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0757e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ABCD Corp alays values ttheir employees!!!\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe50b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddbd582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\" ABCD Corp alays values ttheir employees!!!\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0941e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\" ABCD For always values their employees!!!\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29eab446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"has\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('hasss').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "049a9342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"or\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sometimes it failsas well\n",
    "TextBlob('ur').correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3704f",
   "metadata": {},
   "source": [
    "### Word Count \n",
    "With the help of word count, we can count the frequency of words or a noun phrase in a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1169c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Sentiment Analysis is a process by which we can find the sentiment of a text. Sentiment can be Positive, Negative or Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad31d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b830875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9daa2972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fbf1a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a39c4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"Analysis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3582855",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "With the help of tags function of textblob, we can get tag each words of a sentence with a tag that can be either noun, pronoun, verb, adverb, adjective and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956dad85",
   "metadata": {},
   "source": [
    "\n",
    "Abbreviation\tMeaning\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tThis NLTK POS Tag is an adjective (large)\n",
    "JJR\tadjective, comparative (larger)\n",
    "JJS\tadjective, superlative (largest)\n",
    "LS\tlist market\n",
    "MD\tmodal (could, will)\n",
    "NN\tnoun, singular (cat, tree)\n",
    "NNS\tnoun plural (desks)\n",
    "NNP\tproper noun, singular (sarah)\n",
    "NNPS\tproper noun, plural (indians or americans)\n",
    "PDT\tpredeterminer (all, both, half)\n",
    "POS\tpossessive ending (parent\\ ‘s)\n",
    "PRP\tpersonal pronoun (hers, herself, him, himself)\n",
    "PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "RB\tadverb (occasionally, swiftly)\n",
    "RBR\tadverb, comparative (greater)\n",
    "RBS\tadverb, superlative (biggest)\n",
    "RP\tparticle (about)\n",
    "TO\tinfinite marker (to)\n",
    "UH\tinterjection (goodbye)\n",
    "VB\tverb (ask)\n",
    "VBG\tverb gerund (judging)\n",
    "VBD\tverb past tense (pleaded)\n",
    "VBN\tverb past participle (reunified)\n",
    "VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "WDT\twh-determiner (that, what)\n",
    "WP\twh- pronoun (who)\n",
    "WRB\twh- adverb (how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680e50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Adam', 'NNP'), ('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('read', 'VB'), ('about', 'IN'), ('NLP', 'NNP'), ('I', 'PRP'), ('work', 'VBP'), ('at', 'IN'), ('ABCD', 'NNP'), ('Corp', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    " \n",
    "text = TextBlob(\"My name is Adam. I like to read about NLP. I work at ABCD Corp.\")\n",
    "print(text.tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18a7c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My', 'PRP$')\n",
      "('name', 'NN')\n",
      "('is', 'VBZ')\n",
      "('Adam', 'NNP')\n",
      "('I', 'PRP')\n",
      "('like', 'VBP')\n",
      "('to', 'TO')\n",
      "('read', 'VB')\n",
      "('about', 'IN')\n",
      "('NLP', 'NNP')\n",
      "('I', 'PRP')\n",
      "('work', 'VBP')\n",
      "('at', 'IN')\n",
      "('ABCD', 'NNP')\n",
      "('Corp', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "new_tuple=[]\n",
    "for i in text.tags:\n",
    "    print(i)\n",
    "    if 'VBP' not in i[1]:\n",
    "        new_tuple.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a4260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Adam', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('read', 'VB'),\n",
       " ('about', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('at', 'IN'),\n",
       " ('ABCD', 'NNP'),\n",
       " ('Corp', 'NNP')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2be7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "value=''\n",
    "for i in new_tuple:\n",
    "    value=value+\" \" + \"\".join(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d3a2219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name is Adam I to read about NLP I at ABCD Corp'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570dce4",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee75c7a",
   "metadata": {},
   "source": [
    "* Corpus (or corpora in plural) - Corpus is nothing but a collection of text data. The text maybe in one language or maybe a combination of two or more. \n",
    "\n",
    "* Token - The term \"Token\" is nothing but the total number of words in a text, corpus etc, regardless of their freuqncy of occurrence in the text. Tokens are nothing but a string of contiguous characters which either lies between the two spaces or it lies between a space and punctuation. For Example: Suppose you have the following string : \"abc_123_defg\", if you split it on basis of underscores \"_\" you obtained three tokens : \"abc\", \"123\" and \"defg\".\n",
    "\n",
    "**What is tokenization?**\n",
    "\n",
    "Tokenization is a process of splitting the sentence or corpus into its smalles unit i.e. \"Tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9a0bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "R is a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts. It is free and runs on a variety of platforms, including Windows, Unix, and macOS. It provides an unparalleled platform for programming new statistical methods in an easy and straightforward manner. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d13f6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_object = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b60b83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization of the sample corpus\n",
    "corpus_words = blob_object.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a87be64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['R', 'is', 'a', 'comprehensive', 'statistical', 'and', 'graphical', 'programming', 'language', 'which', 'is', 'fast', 'gaining', 'popularity', 'among', 'data', 'analysts', 'It', 'is', 'free', 'and', 'runs', 'on', 'a', 'variety', 'of', 'platforms', 'including', 'Windows', 'Unix', 'and', 'macOS', 'It', 'provides', 'an', 'unparalleled', 'platform', 'for', 'programming', 'new', 'statistical', 'methods', 'in', 'an', 'easy', 'and', 'straightforward', 'manner'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45a3f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fd6bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences= blob_object.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5528532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " R is a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts.\"),\n",
       " Sentence(\"It is free and runs on a variety of platforms, including Windows, Unix, and macOS.\"),\n",
       " Sentence(\"It provides an unparalleled platform for programming new statistical methods in an easy and straightforward manner.\")]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b598ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f396561",
   "metadata": {},
   "source": [
    "#### Pluralization of words using Textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "547f7446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platforms'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('Platform')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdf342db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platformss'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('Platforms')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6343f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platforms\n",
      "sciences\n",
      "communities\n",
      "etcs\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science. \\n It helps community through blogs, Youtube, GLA,etc.\")\n",
    "for word,pos in blob.tags:\n",
    "    if pos == 'NN':\n",
    "        print (word.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3cb03",
   "metadata": {},
   "source": [
    "#### Lemmatization using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9d5ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: Great | LEMMA: Great | STEM: great\n",
      "ORIGINAL: Learning | LEMMA: Learning | STEM: learn\n",
      "ORIGINAL: is | LEMMA: is | STEM: is\n",
      "ORIGINAL: a | LEMMA: a | STEM: a\n",
      "ORIGINAL: great | LEMMA: great | STEM: great\n",
      "ORIGINAL: platform | LEMMA: platform | STEM: platform\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: learn | LEMMA: learn | STEM: learn\n",
      "ORIGINAL: data | LEMMA: data | STEM: data\n",
      "ORIGINAL: science | LEMMA: science | STEM: scienc\n",
      "ORIGINAL: It | LEMMA: It | STEM: it\n",
      "ORIGINAL: helps | LEMMA: help | STEM: help\n",
      "ORIGINAL: community | LEMMA: community | STEM: commun\n",
      "ORIGINAL: through | LEMMA: through | STEM: through\n",
      "ORIGINAL: blogs | LEMMA: blog | STEM: blog\n",
      "ORIGINAL: Youtube | LEMMA: Youtube | STEM: youtub\n",
      "ORIGINAL: GLA | LEMMA: GLA | STEM: gla\n",
      "ORIGINAL: etc | LEMMA: etc | STEM: etc\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science. \\n It helps community through blogs, Youtube, GLA,etc.\")\n",
    "words = blob.words\n",
    "\n",
    "for word in words:\n",
    "    print(\"ORIGINAL:\", word, \"| LEMMA:\", word.lemmatize(), \"| STEM:\", word.stem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fd4a8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learner'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('learner')\n",
    "w.lemmatize(\"n\") ## n here represents noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d08a8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('learning')\n",
    "w.lemmatize(\"v\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6704615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('booking')\n",
    "w.lemmatize(\"v\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77560711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('books')\n",
    "w.lemmatize(\"n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8324cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "showing off the stop words filtration.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "# converts the words in word_tokens to lower case and then checks whether\n",
    "#they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "filtered_sentence=str(filtered_sentence)\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54dfe009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample sentence  showing stop words filtration '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "clean_sent=re.sub(r'[^\\w\\s]','',filtered_sentence)\n",
    "clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63526e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', 'this', 'is', 'a', 'retweeted', 'tweet', 'by', 'Shivam', 'Bansal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by Shivam Bansal'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    print(words)\n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "output=_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01024e7c",
   "metadata": {},
   "source": [
    "#### n-gram in Textblob\n",
    "\n",
    "An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “not at all”, or “turn off light”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fad2e55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Great Learning is a great platform to learn data science. \n",
       " It helps community through blogs, Youtube, GLA,etc.\")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fbe2828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great']),\n",
       " WordList(['Learning']),\n",
       " WordList(['is']),\n",
       " WordList(['a']),\n",
       " WordList(['great']),\n",
       " WordList(['platform']),\n",
       " WordList(['to']),\n",
       " WordList(['learn']),\n",
       " WordList(['data']),\n",
       " WordList(['science']),\n",
       " WordList(['It']),\n",
       " WordList(['helps']),\n",
       " WordList(['community']),\n",
       " WordList(['through']),\n",
       " WordList(['blogs']),\n",
       " WordList(['Youtube']),\n",
       " WordList(['GLA']),\n",
       " WordList(['etc'])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67a47ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning']),\n",
       " WordList(['Learning', 'is']),\n",
       " WordList(['is', 'a']),\n",
       " WordList(['a', 'great']),\n",
       " WordList(['great', 'platform']),\n",
       " WordList(['platform', 'to']),\n",
       " WordList(['to', 'learn']),\n",
       " WordList(['learn', 'data']),\n",
       " WordList(['data', 'science']),\n",
       " WordList(['science', 'It']),\n",
       " WordList(['It', 'helps']),\n",
       " WordList(['helps', 'community']),\n",
       " WordList(['community', 'through']),\n",
       " WordList(['through', 'blogs']),\n",
       " WordList(['blogs', 'Youtube']),\n",
       " WordList(['Youtube', 'GLA']),\n",
       " WordList(['GLA', 'etc'])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3346e17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is']),\n",
       " WordList(['Learning', 'is', 'a']),\n",
       " WordList(['is', 'a', 'great']),\n",
       " WordList(['a', 'great', 'platform']),\n",
       " WordList(['great', 'platform', 'to']),\n",
       " WordList(['platform', 'to', 'learn']),\n",
       " WordList(['to', 'learn', 'data']),\n",
       " WordList(['learn', 'data', 'science']),\n",
       " WordList(['data', 'science', 'It']),\n",
       " WordList(['science', 'It', 'helps']),\n",
       " WordList(['It', 'helps', 'community']),\n",
       " WordList(['helps', 'community', 'through']),\n",
       " WordList(['community', 'through', 'blogs']),\n",
       " WordList(['through', 'blogs', 'Youtube']),\n",
       " WordList(['blogs', 'Youtube', 'GLA']),\n",
       " WordList(['Youtube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e22ce261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is', 'a']),\n",
       " WordList(['Learning', 'is', 'a', 'great']),\n",
       " WordList(['is', 'a', 'great', 'platform']),\n",
       " WordList(['a', 'great', 'platform', 'to']),\n",
       " WordList(['great', 'platform', 'to', 'learn']),\n",
       " WordList(['platform', 'to', 'learn', 'data']),\n",
       " WordList(['to', 'learn', 'data', 'science']),\n",
       " WordList(['learn', 'data', 'science', 'It']),\n",
       " WordList(['data', 'science', 'It', 'helps']),\n",
       " WordList(['science', 'It', 'helps', 'community']),\n",
       " WordList(['It', 'helps', 'community', 'through']),\n",
       " WordList(['helps', 'community', 'through', 'blogs']),\n",
       " WordList(['community', 'through', 'blogs', 'Youtube']),\n",
       " WordList(['through', 'blogs', 'Youtube', 'GLA']),\n",
       " WordList(['blogs', 'Youtube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4679995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34520501686496574\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 7)\t0.5844829010200651\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (2, 5)\t0.5844829010200651\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 4)\t0.444514311537431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2e7ac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 another\n",
      "1 document\n",
      "2 is\n",
      "3 random\n",
      "4 sample\n",
      "5 text\n",
      "6 third\n",
      "7 this\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\HariniLearn\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,8):\n",
    "    print(i,obj.get_feature_names()[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5af9b",
   "metadata": {},
   "source": [
    "The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i.\n",
    "(0, 1) 0.34520501686496574: In the first document (index 0), the word at column index 1 (which corresponds to \"document\") has a TF-IDF score of approximately 0.345.\n",
    "\n",
    "(0, 4) 0.444514311537431: In the first document (index 0), the word at column index 4 (which corresponds to \"sample\") has a TF-IDF score of approximately 0.445.\n",
    "\n",
    "(0, 2) 0.5844829010200651: In the first document (index 0), the word at column index 2 (which corresponds to \"is\") has a TF-IDF score of approximately 0.584.\n",
    "\n",
    "(1, 3) 0.652490884512534: This means that in the second document (index 1), the word at column index 3 (which corresponds to \"random\") has a TF-IDF score of approximately 0.652.\n",
    "\n",
    "(1, 0) 0.652490884512534: This means that in the same second document (index 1), the word at column index 0 (which corresponds to \"another\") also has a TF-IDF score of approximately 0.652.\n",
    "\n",
    "The reason both \"random\" and \"another\" have the same TF-IDF score in the second document could be due to similar term frequencies and inverse document frequencies for these words. If the term frequencies (how often the word appears in the document) and the inverse document frequency (how unique the word is across the entire corpus) are similar for these words in that specific document, their TF-IDF scores can be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "001b9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02367166\n",
      "-0.11410725\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model1 = Word2Vec(sentences, min_count = 1)#min_count=1 words that appear only once in the training data will also be included in the vocabulary and used to generate word embeddings\n",
    "print (model1.wv.similarity('data', 'science'))\n",
    "print (model1.wv.similarity('vidhya', 'science'))  \n",
    "print (model1.wv.similarity('vidhya', 'vidhya'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def3716",
   "metadata": {},
   "source": [
    "Similarity Scores Near 1: A similarity score close to 1 indicates that the words are very similar in their semantic meaning. They are likely to co-occur frequently in similar contexts and have a strong relationship.\n",
    "\n",
    "Similarity Scores Around 0: A similarity score around 0 suggests that the words are neither very similar nor dissimilar. They might not have a strong relationship in terms of co-occurrence patterns.\n",
    "\n",
    "Similarity Scores Near -1: A similarity score close to -1 indicates that the words are dissimilar and might even have an opposite or contrasting meaning. They are unlikely to co-occur in similar contexts.\n",
    "A positive similarity score indicates that the words are semantically similar or related in meaning. A higher positive value generally indicates a stronger similarity.\n",
    "\n",
    "A negative similarity score, like the one you provided, indicates that the words are less similar or even dissimilar in their semantic meaning. A more negative value suggests a stronger dissimilarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
